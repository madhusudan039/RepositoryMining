{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc44d0a-9ffc-4f18-bf26-53f18423a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules required for the program\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics as stats\n",
    "\n",
    "#Definition of repoMining class\n",
    "class repoMining:\n",
    "    def __init__(self, chrome_driver_address, debug=False):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--use-fake-ui-for-media-stream\")\n",
    "        chrome_options.add_argument(\"--disable-user-media-security=true\")\n",
    "        chrome_options.add_argument('headless')\n",
    "        \n",
    "        self.driver = webdriver.Chrome(chrome_driver_address,options=chrome_options)\n",
    "     \n",
    "        \n",
    "        # Configuring whether output messages will be printed for debugging purpose\n",
    "        self.debug = debug\n",
    "    \n",
    "    # Function to get all the sub-directories or modules under a directory\n",
    "    def getModules(self, url):\n",
    "        module_list = []\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            content = self.driver.page_source\n",
    "            soup = BeautifulSoup(content,'html.parser')\n",
    "        except:\n",
    "            if self.debug == True:\n",
    "                print('Cannot Process URL: ',url)\n",
    "            return []\n",
    "\n",
    "        for element in soup.body.findAll('div',attrs={\"class\":\"Box mb-3\"}):\n",
    "            for level1 in element.findAll('div', attrs={\"role\":\"row\"}):\n",
    "                for tag in level1.findAll('div', attrs={\"role\":\"rowheader\"}):\n",
    "                    text = tag.get_text().strip()\n",
    "                    if self.debug == True:\n",
    "                        print('Text: '+text)\n",
    "\n",
    "                    link = tag.find('a', href=True)\n",
    "                    url = link['href']\n",
    "                    if self.debug == True:\n",
    "                        print(\"URL: \"+url)\n",
    "\n",
    "                    if \"tree\" in url:\n",
    "                        url_split = url.split('/')\n",
    "                        module = url_split[-2] +'/'+url_split[-1]\n",
    "                        module_list.append(module)\n",
    "\n",
    "        return module_list\n",
    "\n",
    "    # getDuration takes a string with commit date, processes the date and returns the difference (in number of days) between\n",
    "    # the processed date and current date\n",
    "    def getDuration(self, str):\n",
    "        str2 = str.split()\n",
    "        str = str2[-3]+\" \"+str2[-2]+\" \"+str2[-1]\n",
    "        commit_datetime = datetime.strptime(str, '%b %d, %Y')\n",
    "        duration = datetime.now() - commit_datetime\n",
    "        return duration.days\n",
    "\n",
    "    # mapCommitInfo function collects the all commit and churn infomation of all the files under the root directory (nova/nova here)\n",
    "    # considering the history log of the root directory. During information collection, only the history within the specified number of days\n",
    "    # are considered. No module to file mapping is done here.\n",
    "    def mapCommitInfo(self, url, github_prefix, number_of_days):\n",
    "        commit_dict = {}\n",
    "        content = None\n",
    "        content2 = None\n",
    "        soup = None\n",
    "        soup2 = None\n",
    "        is_loop =True\n",
    "        # Loop to traverse pages one by one\n",
    "        while is_loop == True: \n",
    "            is_loop = False\n",
    "            \n",
    "            try:\n",
    "                self.driver.delete_all_cookies()\n",
    "                self.driver.get(url)\n",
    "                content = self.driver.page_source\n",
    "                soup = BeautifulSoup(content,'html.parser')\n",
    "            except:\n",
    "                if self.debug == True:\n",
    "                    print('Cannot Process URL: ',url)\n",
    "                return commit_dict, 1\n",
    "\n",
    "            is_next_page = True\n",
    "            # Finding commits of a day\n",
    "            for element in soup.body.findAll('div',attrs={\"class\":\"TimelineItem-body\"}):\n",
    "                header = element.find('h2')\n",
    "                if header is None:\n",
    "                    continue\n",
    "                header_info = header.get_text()\n",
    "                    \n",
    "                if int(self.getDuration(header_info)) <= number_of_days:\n",
    "                    if self.debug == True:\n",
    "                        print(\"Header Information: \"+ header_info)\n",
    "                    print('============= Commits from ',self.getDuration(header_info), \" days ago =============\")\n",
    "                    # Getting all of a commits of a day with corresponding messages\n",
    "                    for link_tag in element.findAll('li'):\n",
    "                        commit_message = link_tag.find('p')\n",
    "                        if commit_message is None:\n",
    "                            continue\n",
    "                        commit_message = commit_message.get_text().strip()\n",
    "                        if self.debug == True:\n",
    "                            print(commit_message)\n",
    "                            \n",
    "                        # Retrieving the url of the commit\n",
    "                        commit_info_url = link_tag.find('a',href=True)\n",
    "                        if commit_info_url is None:\n",
    "                            continue\n",
    "                        commit_info_url= commit_info_url['href']\n",
    "                        commit_info_url = github_prefix + commit_info_url\n",
    "                        \n",
    "                        # Fetching commit and churn info from the commit url\n",
    "                        if self.debug == True:\n",
    "                            print(\"Link: \", commit_info_url)\n",
    "                        try:\n",
    "                            self.driver.delete_all_cookies()\n",
    "                            \n",
    "                            self.driver.get(commit_info_url)\n",
    "                            content2 = self.driver.page_source\n",
    "                            soup2 = BeautifulSoup(content2,'html.parser')\n",
    "                        except:\n",
    "                            if self.debug == True:\n",
    "                                print('Cannot Process URL: ',url)\n",
    "                            return commit_dict, 2\n",
    "\n",
    "                        # Retrieving file level changes (churns) info\n",
    "                        element_i=soup2.body.find('div',attrs={\"id\":\"toc\"})\n",
    "                        if element_i is None:\n",
    "                            continue\n",
    "                        for li_info in element_i.findAll('li'):\n",
    "                            li_list = li_info.get_text().strip().split()\n",
    "                            if self.debug == True:\n",
    "                                print('li_info: ', li_list)\n",
    "\n",
    "                            if len(li_list)!=3:\n",
    "                                continue\n",
    "                            pattern = '\\d+'\n",
    "                            if len(re.findall(pattern, li_list[0]))>0:\n",
    "                                pos_count = int(re.findall(pattern, li_list[0])[0])\n",
    "                            if len(re.findall(pattern, li_list[1]))>0:\n",
    "                                neg_count = int(re.findall(pattern, li_list[1])[0])\n",
    "                            file_name = li_list[2]\n",
    "                            if self.debug == True:\n",
    "                                print(pos_count, neg_count, file_name)\n",
    "                            if file_name not in commit_dict:\n",
    "                                commit_dict[file_name] = []\n",
    "                            commit_dict[file_name].append((pos_count, neg_count))\n",
    "                else:\n",
    "                    return commit_dict, 0\n",
    "\n",
    "            # Retrieving the next page url (if any)\n",
    "            element_older_page= soup.body.find('div',attrs={\"class\":\"paginate-container\"})\n",
    "            if element_older_page is None:\n",
    "                continue\n",
    "            for tag in element_older_page.findAll('a', href=True):\n",
    "                if tag.get_text() == \"Older\":\n",
    "                    url = tag['href']\n",
    "                    print(\"==== Older URL====\", url)\n",
    "                    is_loop = True\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "        return commit_dict, 0\n",
    "    \n",
    "    # reduceCommitInfo aggregates the commit and churn information from file level to module level\n",
    "    def reduceCommitInfo(self, commit_dict, module_list):\n",
    "        commit_count_dict = {}\n",
    "        churn_count_dict = {}\n",
    "        for module in module_list:\n",
    "            commit_count_dict[module] = 0\n",
    "            churn_count_dict[module] = 0\n",
    "            \n",
    "        for file, commit_list in commit_dict.items():\n",
    "            commit_count = len(commit_list)\n",
    "            churn_count = 0\n",
    "            for pos_count, neg_count in commit_list:\n",
    "                churn_count = churn_count + pos_count + neg_count\n",
    "            \n",
    "            for module in module_list:\n",
    "                if file.startswith(module)==True:\n",
    "                    commit_count_dict[module] += commit_count\n",
    "                    churn_count_dict[module] += churn_count\n",
    "                    break\n",
    "        return commit_count_dict, churn_count_dict\n",
    "    \n",
    "    # printAndSave prints and saves the top k information for a dictionary (same function can be used for both commit info and churn info)\n",
    "    def printAndSave(self, topK, count_dict, count_type):\n",
    "        sorted_dict = dict(sorted(count_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "        with open('Top '+str(topK)+' '+count_type+' info','w', encoding=\"utf-8\") as f:\n",
    "            index = 1\n",
    "            for key, value in sorted_dict.items():\n",
    "                print('Rank '+str(index)+' :'+key+' with '+str(value) +' '+count_type+'s')\n",
    "                f.write('Rank '+str(index)+' :'+key+' with '+str(value) +' '+count_type+'s\\n')\n",
    "                index += 1\n",
    "                if index > topK:\n",
    "                    break\n",
    "                    \n",
    "# This function retrieves information from files and generate corresponding graphs                   \n",
    "def generateGraphs(commit_dict_file, temp_churn_file, temp_commit_file):\n",
    "    commit_dict = {}\n",
    "    commit_count_dict = {}\n",
    "    churn_count_dict = {}\n",
    "    with open(commit_dict_file,'r', encoding=\"utf-8\") as fv:\n",
    "        commit_dict=json.load(fv)\n",
    "\n",
    "    with open(temp_commit_file,'r', encoding=\"utf-8\") as tf:\n",
    "        commit_count_dict = json.load(tf)\n",
    "    with open(temp_churn_file,'r', encoding=\"utf-8\") as tf:\n",
    "        churn_count_dict = json.load(tf)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0.15,0.3,0.8,0.65])\n",
    "    x_values = [k.split('/')[1] for k in commit_count_dict.keys()]\n",
    "    y_values = [v for v in commit_count_dict.values()]\n",
    "    sns.barplot(x=x_values,y=y_values)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.ylabel('# of commits')\n",
    "    plt.xlabel('Modules')\n",
    "    plt.show()\n",
    "    fig.savefig('Module-wise commit distribution.pdf')\n",
    "    plt.close()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0.15,0.3,0.8,0.65])\n",
    "    x_values = [k.split('/')[1] for k in churn_count_dict.keys()]\n",
    "    y_values = [v for v in churn_count_dict.values()]\n",
    "    # ax.bar(x_values,y_values)\n",
    "    sns.barplot(x=x_values,y=y_values)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.ylabel('# of churns')\n",
    "    plt.xlabel('Modules')\n",
    "    plt.show()\n",
    "    fig.savefig('Module-wise churn distribution.pdf')\n",
    "    plt.close()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0.15,0.15,0.8,0.8])\n",
    "    file_level_commit_list = [len(v) for v in commit_dict.values()]\n",
    "    sns.histplot(file_level_commit_list, kde=True, binwidth=(max(file_level_commit_list)-min(file_level_commit_list))/25, color='r',)\n",
    "    plt.ylabel('# of files')\n",
    "    plt.xlabel('# of commits')\n",
    "    plt.show()\n",
    "    fig.savefig('File level commit distribution.pdf')\n",
    "    plt.close()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0.15,0.15,0.8,0.8])\n",
    "    file_level_churn_list = [x+y for vl in commit_dict.values() for x, y in vl]\n",
    "    sns.histplot(file_level_churn_list, ax=ax, kde=True, binwidth=(max(file_level_churn_list)-min(file_level_churn_list))/25, color='r')\n",
    "    # sns.distplot(file_level_churn_list, kde=True, color='r')\n",
    "    plt.ylabel('# of files')\n",
    "    plt.xlabel('# of churns')\n",
    "    plt.show()\n",
    "    fig.savefig('File level churn distribution.pdf')\n",
    "    plt.close()\n",
    "\n",
    "# This function retrieves information from files and generate corresponding statistics (median and median absolute deviation)       \n",
    "def generateStat(commit_dict_file, temp_churn_file, temp_commit_file):\n",
    "    commit_dict = {}\n",
    "    commit_count_dict = {}\n",
    "    churn_count_dict = {}\n",
    "    all_data = []\n",
    "    with open(commit_dict_file,'r', encoding=\"utf-8\") as fv:\n",
    "        commit_dict=json.load(fv)\n",
    "\n",
    "    with open(temp_commit_file,'r', encoding=\"utf-8\") as tf:\n",
    "        commit_count_dict = json.load(tf)\n",
    "    with open(temp_churn_file,'r', encoding=\"utf-8\") as tf:\n",
    "        churn_count_dict = json.load(tf)\n",
    "        \n",
    "    commit_data = [value for value in commit_count_dict.values()]\n",
    "    commit_median = stats.median(commit_data)\n",
    "    commit_mad = sum([abs(value-commit_median) for value in commit_data])/len(commit_data)\n",
    "    print(\"Median of Commit Count: \", commit_median)\n",
    "    print(\"Median Absolute Deviation of Commit Count:\", commit_mad)\n",
    "    \n",
    "    churn_data = [value for value in churn_count_dict.values()]\n",
    "    churn_median = stats.median(churn_data)\n",
    "    churn_mad = sum([abs(value-churn_median) for value in churn_data])/len(churn_data)\n",
    "    print(\"Median of Churn Count: \",churn_median)\n",
    "    print(\"Median Absolute Deviation of Churn Count:\",churn_mad)\n",
    "     \n",
    "    \n",
    "def main():\n",
    "    # A compatible chromedriver need to be downloaded and placed in a folder. Moreover, the path to the \n",
    "    # chromedriver need to specified as following\n",
    "    # In windows\n",
    "    chrome_driver_address = \"C:/chromedriver_win32/chromedriver\"\n",
    "    # In linux\n",
    "    # chrome_driver_address = \"/usr/bin/chromedriver\"\n",
    "    github_prefix = \"https://github.com/\"\n",
    "    main_url = \"https://github.com/openstack/nova/tree/master/nova\"\n",
    "    commit_dict_file = 'temp_commit_dict.json'\n",
    "    temp_churn_file = 'temp_churn_count.json'\n",
    "    temp_commit_file = 'temp_commit_count.json'\n",
    "    topK = 12\n",
    "    number_of_days = 180\n",
    "    commit_url = \"https://github.com/openstack/nova/commits/master/nova\"\n",
    "    \n",
    "    rm = repoMining(chrome_driver_address= chrome_driver_address, debug=False)\n",
    "    module_list = rm.getModules(main_url)\n",
    "    print(module_list)\n",
    "\n",
    "    commit_dict, is_problem = rm.mapCommitInfo(commit_url, github_prefix, number_of_days)\n",
    "    print(commit_dict)\n",
    "    with open(commit_dict_file,'w', encoding=\"utf-8\") as cf:\n",
    "            json.dump(commit_dict, cf)\n",
    "    if is_problem != 0:\n",
    "        print('Problem in crawling pages')\n",
    "        return\n",
    "    \n",
    "    commit_count_dict, churn_count_dict = rm.reduceCommitInfo(commit_dict, module_list)\n",
    "    \n",
    "    with open(temp_commit_file,'w', encoding=\"utf-8\") as tf:\n",
    "            json.dump(commit_count_dict, tf)\n",
    "    with open(temp_churn_file,'w', encoding=\"utf-8\") as tf:\n",
    "            json.dump(churn_count_dict, tf)\n",
    "            \n",
    "    total_commits = sum([v for v in commit_count_dict.values()])\n",
    "    total_churns = sum([v for v in churn_count_dict.values()])  \n",
    "    \n",
    "    print(commit_count_dict, churn_count_dict)\n",
    "    print(\"Total number of commits: \", total_commits)\n",
    "    print(\"Total number of churns: \", total_churns)\n",
    "    \n",
    "    rm.printAndSave(topK,commit_count_dict,'commit')\n",
    "    rm.printAndSave(topK,churn_count_dict,'churn')\n",
    "\n",
    "    generateGraphs(commit_dict_file, temp_churn_file, temp_commit_file)\n",
    "    generateStat(commit_dict_file, temp_churn_file, temp_commit_file)\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ec088-fc28-4138-b472-436d1271e92d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
